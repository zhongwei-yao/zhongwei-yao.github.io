{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#zhongwei-yao","title":"Zhongwei Yao (\u200b\u59da\u949f\u200b\u73ae\u200b)","text":"<p>I am an assistant professor at School of Finance, Zhejiang University of Finance and Economics. My research interests include the Chinese mutual funds, institutional investors, and corporate governance. My current work focuses on understanding Chinese mutual fund managers' expectation formation and its implication on asset returns.</p> <p>My work has appeared in Journal of Corporate Finance and Pacific-Basin Finance Journal.</p>"},{"location":"#research-interests","title":"Research Interests","text":"<ul> <li>Mutual funds</li> <li>Institutional investors</li> <li>Corporate governance</li> </ul>"},{"location":"#education","title":"Education","text":"<ul> <li>Ph.D. in Finance, Zhejiang University, 2023 </li> <li>M.S. in Finance, Zhejiang University, 2020 (Master-doctor combind program)</li> <li>B.S. in Mathematics, Zhejiang International Studies University, 2017</li> </ul>"},{"location":"#contact","title":"Contact","text":"<ul> <li>Email: yaozhongwei@zufe.edu.cn</li> <li>Address: School of Finance, Zhejiang University of Finance and Economics, Hangzhou, Chinas</li> </ul>"},{"location":"research/","title":"Research","text":""},{"location":"research/#research","title":"Research","text":""},{"location":"research/#publications","title":"\ud83d\udcc4 Publications","text":"<p>Bubble-crash experience and investment styles of mutual fund managers. Journal of Corporate Finance, 2022, 76, 102262 (with Deming Luo and Yanjian Zhu).</p> <p>Chinese mutual fund managers who experienced a stock market crash are more value-oriented in their portfolios.</p> <p>Economic policy uncertainty and mutual fund risk shifting. Pacific-Basin Finance Journal, 2023, 77, 101921 (with Deming Luo and Sainan Jiang). </p> <p>Mutual fund managers increase risk-shifting activities when uncertainty rises.</p>"},{"location":"research/#working-papers","title":"\ud83d\udcdd Working Papers","text":""},{"location":"research/#in-circulation","title":"In circulation","text":"<p>\"Belief dispersion in the Chinese stock market and fund flows\", with Yue Fang and Deming Luo.</p> <p>We construct a text-based degree of disagreement (DOD) about stock market performance among fund managers using deep learning models.  In the time series, the DOD negatively predicts market returns. In the cross section, fund investors correctly perceive the DOD as an overpricing signal and discount fund performance accordingly. Flow-performance sensitivity is diminished during high dispersion periods.</p>"},{"location":"research/#in-progress","title":"In progress","text":"<ol> <li>\"Stock market expectations and mutual fund performance\", with Yue Fang and Deming Luo.</li> <li>\"Retail investors and mutual fund investment styles\", with Yue Fang and Deming Luo.</li> <li>\"Default risk, external governance, and stock price crash risk\", with  Huaigang Long, Cuixia Tao, and Yanjian Zhu.</li> </ol>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tags","title":"Tags","text":""},{"location":"tags/#oos-analysis","title":"OOS analysis","text":"<ul> <li>Out-of-Sample R-squared</li> </ul>"},{"location":"tags/#return-predictability","title":"Return predictability","text":"<ul> <li>Hodrick (1992) Standard Errors (IB)</li> <li>Out-of-Sample R-squared</li> <li>Hodrick (1992) Standard Errors (Reverse Regression)</li> </ul>"},{"location":"tags/#standard-errors","title":"Standard errors","text":"<ul> <li>Hodrick (1992) Standard Errors (IB)</li> <li>Hodrick (1992) Standard Errors (Reverse Regression)</li> </ul>"},{"location":"posts/","title":"Blog posts","text":""},{"location":"posts/hodrick-1992-standard-errors-ib/","title":"Hodrick (1992) Standard Errors (IB)","text":"","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-ib/#hodrick-1992-standard-errors-ib","title":"Hodrick (1992) Standard Errors (IB)","text":"<p>This post shows how to correct standard errors for a forward predictive regression with overlapping observations as in Hodrick (1992) in R. For more details, please refer to Alex Chinco.</p>","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-ib/#1-predictive-regression-analysis-with-ols","title":"1. Predictive regression analysis with OLS","text":"<p>A typical standard predictive regression model for analyzing aggregate stock market return predictability is given by:</p> \\[ r_{t\\rightarrow t+h} = \\alpha + \\beta\\cdot x_t+\\varepsilon_{t\\rightarrow t+h} \\] <p>where \\(R_{t\\rightarrow t+h}\\) is the \\(h\\)-period ahead cumulative excess market return from period \\(t\\) to \\(t + h\\), and \\(x_t\\) is a predictor.  We can set \\(h=1\\) and vetorize the above equation: </p> \\[ \\underbrace{\\left[\\begin{array}{c} r_{1 \\rightarrow 2} \\\\ r_{2 \\rightarrow 3} \\\\ r_{3 \\rightarrow 4} \\\\ \\vdots \\\\ r_{(T-1) \\rightarrow T} \\end{array}\\right]}_{R_T(1)}=\\underbrace{\\left[\\begin{array}{cc} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ 1 &amp; x_3 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{T-1} \\end{array}\\right]}_{X_{T-1}} \\underbrace{\\left(\\begin{array}{c} \\alpha \\\\ \\beta \\end{array}\\right)}_{\\Theta(1)}+\\underbrace{\\left[\\begin{array}{c} \\varepsilon_{1 \\rightarrow 2} \\\\ \\varepsilon_{2 \\rightarrow 3} \\\\ \\varepsilon_{3 \\rightarrow 4} \\\\ \\vdots \\\\ \\varepsilon_{(T-1) \\rightarrow T} \\end{array}\\right]}_{\\mathcal{E}_{T}(1)}. \\] <p>Because the error term \\(\\varepsilon_{T}(1)\\) is assumed to be \\(i.i.d.\\), we can simply estimate \\(\\Theta(1)\\) with the OLS and use traditional \\(t\\)-statistic to make statistical inferences.</p>","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-ib/#2-econometric-issues","title":"2. Econometric issues","text":"<p>However, for \\(h&gt;1\\), even though each of the \\(\\varepsilon_{t \\to (t+1)}\\) terms is distributed \\(i.i.d.\\) and act as white noise, the \\(\\varepsilon_{t \\to (t+2)}\\) and \\(\\varepsilon_{(t+1) \\to (t+3)}\\) terms each contain the \\(\\varepsilon_{(t+1) \\to (t+2)}\\) shock. We can clearly see this issue from vectorizing the equation:</p> \\[ \\underbrace{\\left[\\begin{array}{c} r_{1 \\rightarrow 3} \\\\ r_{2 \\rightarrow 4} \\\\ r_{3 \\rightarrow 5} \\\\ \\vdots \\\\ r_{(T-2) \\rightarrow T} \\end{array}\\right]}_{R_T(2)}=\\underbrace{\\left[\\begin{array}{cc} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ 1 &amp; x_3 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{T-1} \\end{array}\\right]}_{X_{T-1}} \\underbrace{\\left(\\begin{array}{c} \\alpha \\\\ \\beta \\end{array}\\right)}_{\\Theta(2)}+\\underbrace{\\left[\\begin{array}{c} \\varepsilon_{1 \\rightarrow 3} \\\\ \\varepsilon_{2 \\rightarrow 4} \\\\ \\varepsilon_{3 \\rightarrow 5} \\\\ \\vdots \\\\ \\varepsilon_{(T-2) \\rightarrow T} \\end{array}\\right]}_{\\mathcal{E}_{T}(2)}. \\]","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-ib/#3-hodrick-1992-solution","title":"3. Hodrick (1992) solution","text":"<p>The asymptotic distribution of the OLS estimator \\(\\widehat\\Theta(h)\\) can be derived using GMM:</p> \\[ \\widehat\\Theta(h)-\\Theta(h)\\sim N(0,\\Sigma) \\] <p>with the variance covariance matrix given by the expression: <sup>1</sup> </p> \\[ \\Sigma = \\frac{1}{T-h}(XX^\\prime)^{-1}\\cdot S \\cdot (XX^\\prime)^{-1}. \\] <p>Hodrick (1992) proposes a new estimator to correct for the autocorelation between the error term when the \\(h&gt;1\\):</p> \\[ S=\\frac{1}{T}\\sum_{t=h}^{T}\\left[e_{t+1}^2\\left(\\sum_{i=0}^{h-1} X_{t-i}\\right)\\left(\\sum_{i=0}^{h-1} X_{t-i}\\right)^{\\prime}\\right], \\] <p>where \\(e_{t+1}\\) is the serially uncorrelated one-step-ahead forecast error estimated from the residuals of a regression of \\(r_{t+1}\\) on a constant, and \\(X_t=[1,\\ x_t]\\).</p> <p>By forming </p> \\[ wk_t = e_{t+1}\\left(\\sum_{i=0}^{h-1} X_{t-i}\\right), \\] <p>the covariance matrix \\(S\\) can be written as:</p> \\[ S = \\frac{1}{T}\\sum_{t=h}^{T} wk_t wk_t^\\prime. \\]","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-ib/#4-hodrick-standard-errors-in-r","title":"4. Hodrick standard errors in R","text":"","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-ib/#41-prerequisite","title":"4.1 Prerequisite","text":"<pre><code>library(data.table)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(knitr)\n</code></pre>","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-ib/#42-break-down-hodrick-standard-errors","title":"4.2 Break down Hodrick standard errors","text":"<p>In this subsection, we break down Hodrick standard errors into \\(wk_t\\) and \\(S\\).</p>","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-ib/#421-wk","title":"4.2.1 \\(wk\\)","text":"<pre><code>compute_wk_t &lt;- function(t, h){\n  wk_t &lt;- matrix(0, nrow = K, ncol = 1)\n  XX &lt;- matrix(0, nrow = K, ncol = 1)\n  for (i in 0:(h-1)){\n    XX &lt;- XX + x.mat[t-i,]\n  }\n  wk_t &lt;- ee.mat[t] * XX\n  return(wk_t)\n}\n</code></pre>","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-ib/#422-s","title":"4.2.2 \\(S\\)","text":"<pre><code>compute_S &lt;- function(h){\n  S &lt;- matrix(0, nrow = K, ncol = K)\n  for (t in h:T){\n    S &lt;- S + (compute_wk_t(t,h) %*% t(compute_wk_t(t,h)))\n  }\n  S &lt;- S / T\n  return(S)\n}\n</code></pre>","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-ib/#423-hodrick-variance-covariance-matrix","title":"4.2.3 Hodrick variance-covariance matrix","text":"<pre><code>hodrick1992vcov.forward &lt;- function(x.var, r.var.ahead, h){\n  x.mat &lt;- as.matrix(x.var)\n  r.mat &lt;- as.matrix(r.var.ahead)\n  # 1. Construct demeaned returns or one-period residuals\n  ee.mat &lt;- r.mat - colMeans(r.mat) \n  # ee.mat &lt;- as.matrix(lm(r.var.ahead~1)$residuals) # equivalent to the demeaned returns\n  x.mat &lt;- cbind(1, x.mat)  # add the constant\n  T &lt;- nrow(x.mat)\n  K &lt;- ncol(x.mat)\n  Exx &lt;- t(x.mat) %*% x.mat / T # compute average of square  (1/T) * (X'X)\n  b &lt;- solve(t(x.mat) %*% x.mat) %*% t(x.mat) %*% r.mat\n\n\n  compute_wk_t &lt;- function(t, h){\n    wk_t &lt;- matrix(0, nrow = K, ncol = 1)\n    XX &lt;- matrix(0, nrow = K, ncol = 1)\n    for (i in 0:(h-1)){\n      XX &lt;- XX + x.mat[t-i,]\n    }\n    wk_t &lt;- ee.mat[t] * XX\n    return(wk_t)\n  }\n\n  compute_S &lt;- function(h){\n    S &lt;- matrix(0, nrow = K, ncol = K)\n    for (t in h:T){\n      S &lt;- S + (compute_wk_t(t,h) %*% t(compute_wk_t(t,h)))\n    }\n    S &lt;- S / T\n    return(S)\n  }\n\n\n  S &lt;- compute_S(h)\n  vcov_hodrick &lt;- (1 / T) * solve(Exx) %*% S %*% solve(Exx)\n  std_hodrick &lt;- sqrt(diag(vcov_hodrick))\n  return(list(b = b, vcov_hodrick = vcov_hodrick, std_hodrick = std_hodrick, Nobs = T))\n}\n</code></pre>","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-ib/#43-simulation","title":"4.3 Simulation","text":"<pre><code>simulate_overlapping_data &lt;- function(n_of_years = 100, delta_t = 1/12){\n  MU &lt;- 0.08\n  THETA &lt;- 0.75\n  SIGMA &lt;- 0.16\n\n\n  n_of_periods &lt;- n_of_years / delta_t\n\n  simulate_dt &lt;- data.table(\n    t = seq(1, n_of_periods),\n    x_t = rnorm(n_of_periods, mean = 0, sd = 1),\n    r_t = NA_real_,\n    r_t_plus_1 = NA_real_\n  )\n  #&lt; Set the return of the beginning period to the MU.\n  set(simulate_dt, i = 1L, j = \"r_t\", MU)\n  for (t in 1:(n_of_periods-1)) {\n    t &lt;- as.integer(t)\n    set(simulate_dt, t, \"r_t_plus_1\", THETA * (MU - simulate_dt$r_t[t]) * delta_t + SIGMA * sqrt(delta_t) * rnorm(1))\n    set(simulate_dt, t + 1L, \"r_t\", simulate_dt$r_t_plus_1[t])\n  }\n  simulate_dt[, `:=`(\n    r_t_plus_3 = frollsum(r_t, n = 3, align = \"left\"),\n    r_t_plus_6 = frollsum(r_t, n = 6, align = \"left\"),\n    r_t_plus_12 = frollsum(r_t, n = 12, align = \"left\")\n  )]\n\n  return(simulate_dt[!is.na(r_t_plus_12)])\n}\n\nSIM_N &lt;- 500\nh &lt;- 6\nestimates &lt;- data.table(\n  n = seq(1, SIM_N),\n  beta = NA,\n  se_naive = NA,\n  se_nw = NA,\n  se_hodrick1992 = NA\n)\n\nfor (n in 1:SIM_N) {\n\n  simulate_dt &lt;- simulate_overlapping_data()\n\n  m &lt;- lm(r_t_plus_6 ~ x_t, data = simulate_dt)\n  estimates$beta[n] &lt;- summary(m)$coef[2, 1]\n  estimates$se_naive[n] &lt;- summary(m)$coef[2, 2]\n\n  DF &lt;- summary(m)$df[2]\n  hodrick_vcov &lt;- hodrick1992vcov.forward(simulate_dt$r_t_plus_1, simulate_dt$x_t, h)\n  m_hodrik1992 &lt;- coeftest(m, df = DF, vcov = hodrick_vcov$vcov_hodrick)\n  estimates$se_hodrick1992[n] &lt;- m_hodrik1992[2, 2]\n\n  nw_vcov &lt;- NeweyWest(m, lag = 12, prewhite = FALSE)\n  m_nw &lt;- coeftest(m, df = DF, vcov = nw_vcov)\n  estimates$se_nw[n] &lt;- m_nw[2, 2]\n\n}\n\nkable(copy(estimates)[, n := NULL][, lapply(.SD, mean)][], format = \"pipe\")\n</code></pre> beta se_naive se_nw se_hodrick1992 0.0001972 0.0031126 0.0030742 0.0032698 <ol> <li> <p>Note that in h-step-ahead forecast, we drop the latest h observations. Thus, the sample size is T-h.\u00a0\u21a9</p> </li> </ol>","tags":["Return predictability","Standard errors"]},{"location":"posts/out-of-sample-r-squared/","title":"Out-of-Sample R-squared","text":"","tags":["Return predictability","OOS analysis"]},{"location":"posts/out-of-sample-r-squared/#out-of-sample-r-squared","title":"Out-of-Sample R-squared","text":"<p>This post shows how to evaluate a predictor\u2019s performance using out-of-sample \\(R^2_{OS}\\) in R. For more details, please refer to Campbell and Thompson (2008).</p>","tags":["Return predictability","OOS analysis"]},{"location":"posts/out-of-sample-r-squared/#1-in-sample-r2","title":"1 In-sample \\(R^2\\)","text":"\\[ R^2 = 1- \\frac{\\sum_{t=1}^{T}(r_t - \\hat r_t)^2}{\\sum_{t=1}^{T}(r_t - \\bar r)^2} \\]","tags":["Return predictability","OOS analysis"]},{"location":"posts/out-of-sample-r-squared/#2-out-of-sample-r2","title":"2 Out-of-sample \\(R^2\\)","text":"<p>The \\(R^2_{OS}\\) statistic is proposed by Campbell and Thompson (2008) and measures the proportional reduction in mean squared prediction error (MSPE) for the predictive regression forecast relative to the historical average benchmark:</p> \\[ R^2_{OS} = 1- \\frac{\\sum_{t=1}^{T}(r_t - \\hat r_t)^2}{\\sum_{t=1}^{T}(r_t - \\bar r_t)^2} \\] <p>where \\(\\hat r_t\\) is the fitted value from a predictive regression estimated through period \\(t-1\\), and \\(\\hat r_t\\) is the historical average return estimated through period \\(t-1\\) (i.e., \\(\\hat r_{t+1} =\\frac{1}{t} \\sum_{s=1}^{t}r_t\\)). The \\(R^2_{OS}\\) lies in the range (\\(-\\infty\\),1].</p>","tags":["Return predictability","OOS analysis"]},{"location":"posts/out-of-sample-r-squared/#3-calculate-oos-r-squared-using-r","title":"3 Calculate OOS R-squared using R","text":"","tags":["Return predictability","OOS analysis"]},{"location":"posts/out-of-sample-r-squared/#31-prerequisite","title":"3.1 Prerequisite","text":"<pre><code>library(data.table)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(knitr)\nlibrary(fixest)\nlibrary(broom)\nlibrary(purrr)\nlibrary(multDM)\nlibrary(slider)\n# remotes::install_github(\"franz-maikaefer/oosanalysis-R-library\", ref = \"9b4251b\")\nlibrary(oosanalysis)\n</code></pre>","tags":["Return predictability","OOS analysis"]},{"location":"posts/out-of-sample-r-squared/#32-data-simulation","title":"3.2 Data simulation","text":"<pre><code>simulate_overlapping_data &lt;- function(n_of_years = 100, delta_t = 1/12){\n  MU &lt;- 0.08\n  THETA &lt;- 0.75\n  SIGMA &lt;- 0.16\n\n\n  n_of_periods &lt;- n_of_years / delta_t\n\n  simulate_dt &lt;- data.table(\n    t = seq(1, n_of_periods),\n    x_t = rnorm(n_of_periods, mean = 0, sd = 1),\n    r_t = NA_real_,\n    r_t_plus_1 = NA_real_\n  )\n  #&lt; Set the return of the beginning period to the MU.\n  set(simulate_dt, i = 1L, j = \"r_t\", MU)\n  for (t in 1:(n_of_periods-1)) {\n    t &lt;- as.integer(t)\n    set(simulate_dt, t, \"r_t_plus_1\", THETA * (MU - simulate_dt$r_t[t]) * delta_t + SIGMA * sqrt(delta_t) * rnorm(1))\n    set(simulate_dt, t + 1L, \"r_t\", simulate_dt$r_t_plus_1[t])\n  }\n  simulate_dt[, `:=`(\n    r_t_plus_3 = frollsum(r_t, n = 3, align = \"left\"),\n    r_t_plus_6 = frollsum(r_t, n = 6, align = \"left\"),\n    r_t_plus_12 = frollsum(r_t, n = 12, align = \"left\")\n  )]\n\n  return(simulate_dt[!is.na(r_t_plus_12)])\n}\n\nset.seed(42)\nsimulate_dt &lt;- simulate_overlapping_data()\nkable(head(simulate_dt), \"pipe\")\n</code></pre> t x_t r_t r_t_plus_1 r_t_plus_3 r_t_plus_6 r_t_plus_12 1 1.3709584 0.0800000 -0.0344801 0.0543657 0.1392246 0.1188966 2 -0.5646982 -0.0344801 0.0088458 -0.0062542 0.1045764 0.0885166 3 0.3631284 0.0088458 0.0193802 0.0495512 0.0290402 0.1400495 4 0.6328626 0.0193802 0.0213252 0.0848590 0.1118831 0.1849328 5 0.4042683 0.0213252 0.0441536 0.1108306 0.1128342 0.1644087 6 -0.1061245 0.0441536 0.0453517 -0.0205110 0.0688945 0.1369073","tags":["Return predictability","OOS analysis"]},{"location":"posts/out-of-sample-r-squared/#33-in-sample-r2","title":"3.3 In-sample \\(R^2\\)","text":"<pre><code>#&lt; Fit the predictive regression\nm &lt;- feols(r_t_plus_1 ~ x_t,simulate_dt)\n\n#&lt; Extract in-sample R-squared from the model\nr_is_1 &lt;- r2(m)[[\"r2\"]]\n\n#&lt; Compute fitted values\nr_t_plus_1_hat &lt;- predict(m)\n\n#&lt; Calculate in-sample R-squared manually\nr_is_2 &lt;- 1 - sum((simulate_dt$r_t_plus_1 - r_t_plus_1_hat)^2) / sum((simulate_dt$r_t_plus_1 - mean(simulate_dt$r_t_plus_1))^2)\n\n#&lt; Present results\nkable(data.table(r_is_1 = r_is_1, r_is_2 = r_is_2))\n</code></pre> r_is_1 r_is_2 0.000549 0.000549","tags":["Return predictability","OOS analysis"]},{"location":"posts/out-of-sample-r-squared/#34-out-of-sample-r2","title":"3.4 Out-of-sample \\(R^2\\)","text":"<pre><code>extract_r2 &lt;- compose(\\(x) x[[\"r2\"]], r2)\npredict_one_step_ahead &lt;- function(object, x){\n  estimates &lt;- coef(object)\n  y_hat &lt;- x * estimates[[2]] + estimates[[1]]\n  return(y_hat)\n}\n\nr2_oos &lt;- function(n.start = NULL, r.var.ahead = NULL, r.var = NULL, x.var = NULL, date.var = NULL, refit.window = c(\"recursive\", \"rolling\"), data = NULL){\n  dt &lt;- as.data.table(data)\n  dt &lt;- dt[, .SD, .SDcols = c(date.var, r.var.ahead, r.var, x.var)]\n  N &lt;- nrow(data)\n  fm &lt;- as.formula(paste0(r.var.ahead, \"~\", x.var))\n  if (refit.window == \"recursive\"){\n    dt_test &lt;- dt[, \n      {\n        slide_dt &lt;- slide(.SD, .f = ~.x, .before = Inf, .after = -1)\n        results &lt;- data.table(.SD, slide_dt)\n        results[n.start:.N]\n      }\n    ]\n  }\n  if (refit.window == \"rolling\"){\n    dt_test &lt;- dt[, \n      {\n        slide_dt &lt;- slide(.SD, .f = ~.x, .before = n.start, .after = -1, .complete = T)\n        results &lt;- data.table(.SD, slide_dt)\n        results[(n.start+1):.N]\n      }\n    ]\n  }\n  #&lt; Define functions for Clark and West (2007) test \n  cw.test &lt;- function(e_null, e_alt, y_hat_null, y_hat_alt, h){\n    P &lt;- length(e_null)\n    mspe.adj &lt;- e_null^2 - (e_alt^2 - (y_hat_null - y_hat_alt)^2)\n    m &lt;- lm(mspe.adj~1)\n    cw_iid &lt;- tidy(m)$statistic\n    cw_nw &lt;- coeftest(m, vcov. = NeweyWest(m, lag = h, prewhite = FALSE))[3]\n    data.table(cw_iid, cw_nw)  \n  }\n\n  dt_test[\n    ,\n    {\n      m = map(slide_dt, ~feols(fm, .x))\n      r_hat_ols = map2_dbl(m, get(x.var), ~predict_one_step_ahead(.x, .y))\n      r_hat_hist_mean = map_dbl(slide_dt, ~mean(.x[, .SD, .SDcols = r.var][[1]]))\n      r_true = get(r.var)\n      e_ols &lt;- r_true - r_hat_ols\n      e_hist_mean &lt;- r_true - r_hat_hist_mean\n      r2_os = 1 - sum((r_true - r_hat_ols)^2) / sum((r_true - r_hat_hist_mean)^2)\n      cw.stats = cw.test(e_hist_mean, e_ols, r_hat_hist_mean, r_hat_ols, 1)\n      dm.stats &lt;- DM.test(r_hat_hist_mean, r_hat_ols, r_true, h = 1, c = TRUE, H1 = \"less\")$statistic[1]\n      # dm.stats2 &lt;- DM.test(r_hat_hist_mean, r_hat_ols, r_true, h = 1, c = TRUE, H1 = \"less\")$statistic[1]\n      data.table(data = list(data.table(get(date.var), r_hat_ols, r_hat_hist_mean, r_true, e_ols, e_hist_mean)), r2_os = r2_os[1], \n                 cw.stats, dm.stats)\n    }\n  ]\n}\n\noos_dt &lt;- r2_oos(n.start = 501, r.var.ahead = \"r_t_plus_1\", r.var = \"r_t\", x.var = \"x_t\", refit.window = \"recursive\", data = simulate_dt)\n\nkable(oos_dt[, .(r2_os, cw_iid, dm.stats)], \"pipe\")\n</code></pre> r2_os cw_iid dm.stats -0.005876 0.5118299 -0.7765246","tags":["Return predictability","OOS analysis"]},{"location":"posts/hodrick-1992-standard-errors-reverse-regression/","title":"Hodrick (1992) Standard Errors (Reverse Regression)","text":"","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-reverse-regression/#hodrick-1992-standard-errors-reverse-regression","title":"Hodrick (1992) Standard Errors (Reverse Regression)","text":"<p>In the previous post, I calculate Hodrick (1992) standard error IB in a forward regression setting. This post shows how to use the reverse regression approach of Hodrick (1992) to compute standard errors which account for the overlapping nature of predictors.</p>","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-reverse-regression/#prerequisite","title":"Prerequisite","text":"<pre><code>library(data.table)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(knitr)\n</code></pre>","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-reverse-regression/#1-hodrick-1992-standard-errors","title":"1 Hodrick (1992) standard errors","text":"","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-reverse-regression/#11-reverse-regression-approach","title":"1.1 Reverse regression approach","text":"<p>The reverse regression of the one-period-ahead return on the previous \\(h\\)-period sum of the predictor is given by:  </p> <pre><code>hodrick1992vcov.reverse &lt;- function(x.var , r.var.ahead, h){\n  x.mat &lt;- as.matrix(x.var)\n  r.mat &lt;- as.matrix(r.var.ahead)\n  # 1. Construct demeaned returns or one-period residuals\n  ee.mat &lt;- r.mat - colMeans(r.mat) \n  # ee.mat &lt;- as.matrix(lm(r.var~1)$residuals) # equivalent to the demeaned returns\n\n  # 2. Construct sum of squares of predictors\n  x.mat &lt;- cbind(1, x.mat)  # add the constant\n  T &lt;- nrow(x.mat)\n  K &lt;- ncol(x.mat)\n  Exx &lt;- t(x.mat) %*% x.mat / T # compute average of square  (1/T) * (X'X)\n  # Construct the reverse regressor by summing previous h-period values \n  rev.x.mat = as.matrix(frollsum(x.var, n = h, align = \"right\"))\n\n  # Construct the matrix for sum of squares and standard errors\n  rev.x.mat &lt;- cbind(matrix(h,T - h +1 ,1) , rev.x.mat[h:T,])  # add the constant and get rid of NAs\n  rev.ee.mat &lt;- as.matrix(ee.mat[h:T,])\n  b_reverse &lt;- solve(t(rev.x.mat) %*% rev.x.mat) %*% t(rev.x.mat) %*% r.mat[h:T,]\n\n  # Compute the matrix of sum of squares and directly the standard errors\n  wk &lt;- rev.ee.mat[,1] * rev.x.mat\n  S  &lt;- 1/nrow(wk) * t(wk) %*% wk\n  vcov_hodrick &lt;- (1 / nrow(wk)) * solve(Exx) %*% S %*% solve(Exx)\n  std_hodrick &lt;- sqrt(diag(vcov_hodrick))\n  return(list(b = b_reverse, vcov_hodrick = vcov_hodrick, std_hodrick = std_hodrick, Nobs = nrow(wk)))\n}\n</code></pre>","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-reverse-regression/#12-forward-regression-approach","title":"1.2 Forward regression approach","text":"<p>As mentioned in the previous post, the standard long-horizon predictive regression is given by:</p> \\[ r_{t \\rightarrow t+h}=\\alpha+\\beta \\cdot x_t+\\varepsilon_{t \\rightarrow t+h}. \\] <pre><code>hodrick1992vcov.forward &lt;- function(x.var , r.var.ahead, h){\n  x.mat &lt;- as.matrix(x.var)\n  r.mat &lt;- as.matrix(r.var.ahead)\n  # 1. Construct demeaned returns or one-period residuals\n  ee.mat &lt;- r.mat - colMeans(r.mat) \n  # ee.mat &lt;- as.matrix(lm(r.var.ahead~1)$residuals) # equivalent to the demeaned returns\n  x.mat &lt;- cbind(1, x.mat)  # add the constant\n  T &lt;- nrow(x.mat)\n  K &lt;- ncol(x.mat)\n  Exx &lt;- t(x.mat) %*% x.mat / T # compute average of square  (1/T) * (X'X)\n  b &lt;- solve(t(x.mat) %*% x.mat) %*% t(x.mat) %*% r.mat\n\n\n  compute_wk_t &lt;- function(t, h){\n    wk_t &lt;- matrix(0, nrow = K, ncol = 1)\n    XX &lt;- matrix(0, nrow = K, ncol = 1)\n    for (i in 0:(h-1)){\n      XX &lt;- XX + x.mat[t-i,]\n    }\n    wk_t &lt;- ee.mat[t] * XX\n    return(wk_t)\n  }\n\n  compute_S &lt;- function(h){\n    S &lt;- matrix(0, nrow = K, ncol = K)\n    for (t in h:T){\n      S &lt;- S + (compute_wk_t(t,h) %*% t(compute_wk_t(t,h)))\n    }\n    S &lt;- S / T\n    return(S)\n  }\n\n\n  S &lt;- compute_S(h)\n  vcov_hodrick &lt;- (1 / T) * solve(Exx) %*% S %*% solve(Exx)\n  std_hodrick &lt;- sqrt(diag(vcov_hodrick))\n  return(list(b = b, vcov_hodrick = vcov_hodrick, std_hodrick = std_hodrick, Nobs = T))\n}\n</code></pre>","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-reverse-regression/#13-combine-these-two-methods","title":"1.3 Combine these two methods","text":"<pre><code>hodrick1992vcov &lt;- function(x.var , r.var.ahead, h, method = c(\"forward\", \"reverse\")){\n\n  if (method == \"forward\"){\n    return(hodrick1992vcov.forward(x.var, r.var.ahead, h))\n  }\n\n  if (method == \"reverse\"){\n    return(hodrick1992vcov.reverse(x.var, r.var.ahead, h))\n  }\n}\n</code></pre>","tags":["Return predictability","Standard errors"]},{"location":"posts/hodrick-1992-standard-errors-reverse-regression/#2-examples","title":"2 Examples","text":"<pre><code>set.seed(42)\nsimulate_dt &lt;- simulate_overlapping_data()\nkable(head(simulate_dt))\n</code></pre> t x_t r_t r_t_plus_1 r_t_plus_3 r_t_plus_6 r_t_plus_12 1 1.3709584 0.0800000 -0.0344801 0.0543657 0.1392246 0.1188966 2 -0.5646982 -0.0344801 0.0088458 -0.0062542 0.1045764 0.0885166 3 0.3631284 0.0088458 0.0193802 0.0495512 0.0290402 0.1400495 4 0.6328626 0.0193802 0.0213252 0.0848590 0.1118831 0.1849328 5 0.4042683 0.0213252 0.0441536 0.1108306 0.1128342 0.1644087 6 -0.1061245 0.0441536 0.0453517 -0.0205110 0.0688945 0.1369073 <pre><code>h &lt;- 6\nm_ols &lt;- lm(simulate_dt$r_t_plus_6~simulate_dt$x_t)\nm_forward &lt;- hodrick1992vcov(x.var = simulate_dt$x_t, r.var.ahead = simulate_dt$r_t_plus_1, h = h, method = \"forward\")\nm_reverse &lt;- hodrick1992vcov(x.var = simulate_dt$x_t, r.var.ahead = simulate_dt$r_t_plus_1, h = h, method = \"reverse\")\nkable(data.table(\n  b_ols = coef(m_ols)[[2]],\n  se_ols = sqrt(diag(vcov(m_ols)))[[2]],\n  se_forward = m_forward$std_hodrick[2],\n  b_reverse = m_reverse$b[2,],\n  se_reverse = m_reverse$std_hodrick[2]\n))\n</code></pre> b_ols se_ols se_forward b_reverse se_reverse 0.0027597 0.0031609 0.0033485 0.0007351 0.0033626","tags":["Return predictability","Standard errors"]},{"location":"posts/archive/2023/","title":"2023","text":""},{"location":"posts/category/teaching-notes/","title":"Teaching Notes","text":""},{"location":"posts/category/research-notes/","title":"Research Notes","text":""},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tags","title":"Tags","text":""},{"location":"tags/#oos-analysis","title":"OOS analysis","text":"<ul> <li>Out-of-Sample R-squared</li> </ul>"},{"location":"tags/#return-predictability","title":"Return predictability","text":"<ul> <li>Hodrick (1992) Standard Errors (IB)</li> <li>Out-of-Sample R-squared</li> <li>Hodrick (1992) Standard Errors (Reverse Regression)</li> </ul>"},{"location":"tags/#standard-errors","title":"Standard errors","text":"<ul> <li>Hodrick (1992) Standard Errors (IB)</li> <li>Hodrick (1992) Standard Errors (Reverse Regression)</li> </ul>"}]}